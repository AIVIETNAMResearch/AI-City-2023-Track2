{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"./data/AIC23_Track2_NL_Retrieval/data/train_nl_extracted_.json\", \"r\") as f:\n",
    "    data_train = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data_df = pd.DataFrame(data_train).transpose().reset_index()\n",
    "data_df = data_df.rename(columns={'index': 'uuid'})\n",
    "data_df['colors'] = data_df['colors'].replace(['white', 'black', 'gray', 'red', 'blue', 'green', 'brown', 'yellow'], range(8))\n",
    "data_df['type'] = data_df['type'].replace(['sedan', 'truck', 'suv', 'van', 'bus', 'hatchback'], range(6))\n",
    "data_df['motion'] = data_df['motion'].replace(['straight', 'left', 'right', 'stop'], range(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>uuid</th>\n",
       "      <th>frames</th>\n",
       "      <th>boxes</th>\n",
       "      <th>nl</th>\n",
       "      <th>nl_other_views</th>\n",
       "      <th>type</th>\n",
       "      <th>motion</th>\n",
       "      <th>colors</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>b06c903c-a25d-45fe-b0d5-294f72e34023</td>\n",
       "      <td>[./validation/S02/c006/img1/000001.jpg, ./vali...</td>\n",
       "      <td>[[539, 606, 273, 277], [532, 631, 271, 282], [...</td>\n",
       "      <td>[A red sedan drives forward., A red midsize se...</td>\n",
       "      <td>[A red sedan keeping straight., A red sedan ru...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3a02a86d-154b-4ee2-bd5e-a0811113a9d8</td>\n",
       "      <td>[./validation/S02/c007/img1/000001.jpg, ./vali...</td>\n",
       "      <td>[[1292, 359, 403, 161], [1230, 360, 406, 160],...</td>\n",
       "      <td>[A red sedan keeping straight., A red sedan dr...</td>\n",
       "      <td>[A red sedan runs down the street followed by ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>eb184f7c-35c7-4f3e-af52-1db57fb087d4</td>\n",
       "      <td>[./validation/S02/c009/img1/000015.jpg, ./vali...</td>\n",
       "      <td>[[1713, 410, 205, 133], [1683, 404, 218, 133],...</td>\n",
       "      <td>[A red sedan runs down the street followed by ...</td>\n",
       "      <td>[A red midsize sedan keep straight., A red car...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>abd32535-8acb-49c5-8da4-9e904d263d8c</td>\n",
       "      <td>[./validation/S02/c006/img1/000001.jpg, ./vali...</td>\n",
       "      <td>[[374, 373, 249, 219], [372, 383, 238, 219], [...</td>\n",
       "      <td>[A white pickup goes straight., A white truck ...</td>\n",
       "      <td>[A pickup truck is going straight., White pick...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3819f85b-103a-4f0b-b0f1-49e6b2fedb68</td>\n",
       "      <td>[./validation/S02/c007/img1/000011.jpg, ./vali...</td>\n",
       "      <td>[[1452, 294, 431, 176], [1367, 294, 454, 178],...</td>\n",
       "      <td>[A white truck runs down the street., A pickup...</td>\n",
       "      <td>[White dodge ram pickup truck going straight t...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2150</th>\n",
       "      <td>ab54d5c1-b712-4baf-9e8b-af4ff8808141</td>\n",
       "      <td>[./validation/S05/c036/img1/002380.jpg, ./vali...</td>\n",
       "      <td>[[1622, 294, 298, 284], [1619, 281, 301, 284],...</td>\n",
       "      <td>[A black sedan switch lane to right and follow...</td>\n",
       "      <td>[A black sedan making a right turn at intersec...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2151</th>\n",
       "      <td>23896dd5-a992-4587-b1e9-acfcf864fdb1</td>\n",
       "      <td>[./validation/S05/c035/img1/002965.jpg, ./vali...</td>\n",
       "      <td>[[1691, 504, 228, 237], [1686, 501, 232, 241],...</td>\n",
       "      <td>[A gray sedan stopped at the intersection., A ...</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2152</th>\n",
       "      <td>2a5e81fb-27d3-4d52-8c4c-e97be079b266</td>\n",
       "      <td>[./validation/S05/c036/img1/003076.jpg, ./vali...</td>\n",
       "      <td>[[1423, 480, 497, 417], [1411, 464, 509, 428],...</td>\n",
       "      <td>[A gray sedan stopped at the intersection., A ...</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2153</th>\n",
       "      <td>a3b55866-1ec7-42a0-bbac-7c6f98ec14bc</td>\n",
       "      <td>[./validation/S05/c035/img1/003460.jpg, ./vali...</td>\n",
       "      <td>[[1124, 0, 109, 84], [1121, 0, 111, 83], [1118...</td>\n",
       "      <td>[A gray SUV stops at the intersection., A gray...</td>\n",
       "      <td>[A gray hatchback going straight down the stre...</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2154</th>\n",
       "      <td>8dc1f323-20bf-41e2-8350-cd2941e629a6</td>\n",
       "      <td>[./validation/S05/c036/img1/003284.jpg, ./vali...</td>\n",
       "      <td>[[1118, 219, 146, 103], [1113, 223, 121, 105],...</td>\n",
       "      <td>[After turning left., A gray hatchback going s...</td>\n",
       "      <td>[A gray SUV stops at the intersection., A gray...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2155 rows Ã— 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      uuid  ... colors\n",
       "0     b06c903c-a25d-45fe-b0d5-294f72e34023  ...      3\n",
       "1     3a02a86d-154b-4ee2-bd5e-a0811113a9d8  ...      3\n",
       "2     eb184f7c-35c7-4f3e-af52-1db57fb087d4  ...      3\n",
       "3     abd32535-8acb-49c5-8da4-9e904d263d8c  ...      0\n",
       "4     3819f85b-103a-4f0b-b0f1-49e6b2fedb68  ...      0\n",
       "...                                    ...  ...    ...\n",
       "2150  ab54d5c1-b712-4baf-9e8b-af4ff8808141  ...      1\n",
       "2151  23896dd5-a992-4587-b1e9-acfcf864fdb1  ...      2\n",
       "2152  2a5e81fb-27d3-4d52-8c4c-e97be079b266  ...      2\n",
       "2153  a3b55866-1ec7-42a0-bbac-7c6f98ec14bc  ...      2\n",
       "2154  8dc1f323-20bf-41e2-8350-cd2941e629a6  ...      1\n",
       "\n",
       "[2155 rows x 8 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import numpy as np\n",
    "from dataloader.utils import get_motion_img\n",
    "import os\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "class Track2CustomDataset(Dataset):\n",
    "    def __init__(self, video_params, data_tracks, tokenizer, max_len, transforms, config, mode=\"train\"):\n",
    "        \n",
    "        self.samples = data_tracks\n",
    "        self.transforms = transforms\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        self.video_params = video_params\n",
    "        self.mode = mode\n",
    "        self.config = config\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        sample = self.samples.iloc[index]\n",
    "\n",
    "        if self.mode == \"train\":\n",
    "\n",
    "            final, motion, motion_line = self.image_features(sample)\n",
    "            text_inputs = self.lang_features(sample)\n",
    "            \n",
    "            sample = {\n",
    "                'text': text_inputs,\n",
    "                'video': final,\n",
    "                'motion': motion,\n",
    "                'motion_line': motion_line,\n",
    "                'color_label': sample['colors'],\n",
    "                'type_label': sample['type'],\n",
    "                'motion_label': sample['motion']\n",
    "\n",
    "            }\n",
    "\n",
    "            return sample\n",
    "        \n",
    "        if self.mode == \"infer_text\":\n",
    "\n",
    "            text_inputs = self.lang_features(sample)\n",
    "            return {'text': text_inputs}\n",
    "        \n",
    "        if self.mode == \"infer_video\":\n",
    "            final, motion, motion_line = self.image_features(sample)\n",
    "            return {\n",
    "                'video': final,\n",
    "                'motion': motion,\n",
    "                'motion_line': motion_line\n",
    "            }    \n",
    "    \n",
    "    def image_features(self, sample):\n",
    "        frames_path, boxes = sample['frames'], sample['boxes']\n",
    "            \n",
    "        veh_imgs, motion_line, motion = get_motion_img(self.config['general_config']['data_dir'], frames_path, boxes, self.config['arch']['base_settings']['video_params']['num_frames'])\n",
    "\n",
    "        if self.transforms:\n",
    "            veh_imgs = [self.transforms(img.astype(np.float32)) for img in veh_imgs]\n",
    "            motion_line = self.transforms(motion_line.astype(np.float32))\n",
    "            motion = self.transforms(motion.astype(np.float32))\n",
    "    \n",
    "        veh_imgs = torch.stack(veh_imgs)\n",
    "        \n",
    "        final = torch.zeros([self.video_params['num_frames'], 3, self.video_params['input_res'], self.video_params['input_res']])\n",
    "        final[: veh_imgs.shape[0]] = veh_imgs\n",
    "\n",
    "        return final, motion, motion_line\n",
    "\n",
    "    def lang_features(self, sample):\n",
    "        nl_descriptions = sample['nl']        \n",
    "        text_inputs = []\n",
    "        for idx, text in enumerate(nl_descriptions):\n",
    "            # print(\"text: \", text, \", idx: \", idx)\n",
    "            tokenized_inp = self.tokenizer.encode_plus(\n",
    "                                text,\n",
    "                                truncation=True,\n",
    "                                add_special_tokens=True,\n",
    "                                max_length=self.max_len,\n",
    "                                padding='max_length'\n",
    "                            )\n",
    "            text_inputs.append({\n",
    "                'input_ids': torch.LongTensor(tokenized_inp['input_ids']),\n",
    "                'attention_mask': torch.LongTensor(tokenized_inp['input_ids'])\n",
    "            })        \n",
    "        \n",
    "        return text_inputs\n",
    "\n",
    "def videotext_collate_fn(batch_data):\n",
    "    print(batch_data[0]['color_label'])\n",
    "    frames = torch.stack([item['video'] for item in batch_data])\n",
    "    motion = torch.stack([item['motion'] for item in batch_data])\n",
    "    motion_line = torch.stack([item['motion_line'] for item in batch_data])\n",
    "    input_ids = torch.stack([cap['input_ids'] for item in batch_data for cap in item['text']])\n",
    "    attention_mask = torch.stack([cap['attention_mask'] for item in batch_data for cap in item['text']])\n",
    "    \n",
    "    color_label = torch.LongTensor([item['color_label'] for item in batch_data])\n",
    "    type_label = torch.LongTensor([item['type_label'] for item in batch_data])\n",
    "    motion_label = torch.LongTensor([item['motion_label'] for item in batch_data])\n",
    "\n",
    "    return {'video': frames, 'text': {'input_ids': input_ids, 'attention_mask': attention_mask}, 'motion': motion, 'motion_line': motion_line,\n",
    "            'color_label': color_label, 'type_label': type_label, 'motion_label': motion_label}\n",
    "\n",
    "def text_collate_fn(batch_data):\n",
    "    input_ids = torch.stack([cap['input_ids'] for item in batch_data for cap in item['text']])\n",
    "    attention_mask = torch.stack([cap['attention_mask'] for item in batch_data for cap in item['text']])\n",
    "    \n",
    "    return {'text': {'input_ids': input_ids, 'attention_mask': attention_mask}}\n",
    "\n",
    "def video_collate_fn(batch_data):\n",
    "    frames = torch.stack([item['video'] for item in batch_data])\n",
    "    motion = torch.stack([item['motion'] for item in batch_data])\n",
    "    motion_line = torch.stack([item['motion_line'] for item in batch_data])\n",
    "    return {'video': frames, 'motion': motion, 'motion_line': motion_line}\n",
    "\n",
    "\n",
    "\n",
    "def get_transforms(img_size, train, size=1):\n",
    "    if train:\n",
    "        return transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.RandomResizedCrop(img_size * size, scale=(0.8, 1)),\n",
    "            transforms.RandomApply([transforms.RandomRotation(10)], p=0.5),\n",
    "            transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
    "        ])\n",
    "    else:\n",
    "        return transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Resize((img_size * size, img_size * size)),\n",
    "            transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
    "        ])\n",
    "\n",
    "\n",
    "def get_train_dataloader(config, df):\n",
    "    dataset = Track2CustomDataset(data_tracks=df, \n",
    "                                  video_params=config.arch.base_settings.video_params,\n",
    "                                  tokenizer=config.general_config.tokenizer,\n",
    "                                  max_len=int(config.general_config.max_len),\n",
    "                                  transforms=get_transforms(config.arch.base_settings.video_params.input_res, train=True),\n",
    "                                  config=config)\n",
    "    dataloader = torch.utils.data.DataLoader(\n",
    "        dataset,\n",
    "        batch_size=config.general_config.train_batch_size,\n",
    "        num_workers=config.general_config.n_workers,\n",
    "        collate_fn=videotext_collate_fn,\n",
    "        shuffle=True,\n",
    "        pin_memory=True,\n",
    "        drop_last=True\n",
    "    )\n",
    "\n",
    "    return dataloader\n",
    "    \n",
    "def get_valid_dataloader(config, df):\n",
    "    dataset = Track2CustomDataset(data_tracks=df, \n",
    "                                  video_params=config.arch.base_settings.video_params,\n",
    "                                  tokenizer=config.general_config.tokenizer,\n",
    "                                  max_len=int(config.general_config.max_len),\n",
    "                                  transforms=get_transforms(config.arch.base_settings.video_params.input_res, train=False),\n",
    "                                  config=config)\n",
    "    dataloader = torch.utils.data.DataLoader(\n",
    "        dataset,\n",
    "        batch_size=config.general_config.valid_batch_size,\n",
    "        num_workers=config.general_config.n_workers,\n",
    "        collate_fn=videotext_collate_fn,\n",
    "        shuffle=False,\n",
    "        pin_memory=True,\n",
    "        drop_last=False\n",
    "    )\n",
    "\n",
    "    return dataloader\n",
    "\n",
    "def get_infer_dataloader(config, df_video, df_text):\n",
    "    text_dataset = Track2CustomDataset(data_tracks=df_text,\n",
    "                                       video_params=config.arch.base_settings.video_params,\n",
    "                                       tokenizer=config.general_config.tokenizer,\n",
    "                                       max_len=int(config.general_config.max_len),\n",
    "                                       transforms=get_transforms(config.arch.base_settings.video_params.input_res, train=False),\n",
    "                                       config=config,\n",
    "                                       mode=\"infer_text\")\n",
    "    \n",
    "    video_dataset = Track2CustomDataset(data_tracks=df_video,\n",
    "                                        video_params=config.arch.base_settings.video_params,\n",
    "                                        tokenizer=config.general_config.tokenizer,\n",
    "                                        max_len=int(config.general_config.max_len),\n",
    "                                        transforms=get_transforms(config.arch.base_settings.video_params.input_res, train=False),\n",
    "                                        config=config,\n",
    "                                        mode=\"infer_video\")\n",
    "    \n",
    "    text_dataloader = torch.utils.data.DataLoader(\n",
    "        text_dataset,\n",
    "        batch_size=config.general_config.valid_batch_size,\n",
    "        num_workers=config.general_config.n_workers,\n",
    "        collate_fn=text_collate_fn,\n",
    "        shuffle=False,\n",
    "        pin_memory=True,\n",
    "        drop_last=False\n",
    "    )\n",
    "\n",
    "    video_dataloader = torch.utils.data.DataLoader(\n",
    "        video_dataset,\n",
    "        batch_size=config.general_config.valid_batch_size,\n",
    "        num_workers=config.general_config.n_workers,\n",
    "        collate_fn=video_collate_fn,\n",
    "        shuffle=False,\n",
    "        pin_memory=True,\n",
    "        drop_last=False\n",
    "    )\n",
    "    return video_dataloader, text_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1])"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.stack([torch.tensor(1) for i in range(10)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'general_config': {'data_dir': './data/AIC23_Track2_NL_Retrieval/data', 'max_len': 64, 'train_batch_size': 12, 'valid_batch_size': 12, 'n_workers': 8, 'kfolds': 5, 'gradient_checkpointing': True, 'epochs': 10, 'n_warmup_steps': 0, 'gradient_accumulation_steps': 1, 'unscale': True, 'evaluate_n_times_per_epoch': 1, 'max_grad_norm': 1000, 'train_print_frequency': 20, 'valid_print_frequency': 20, 'loss': 'InfoNCE', 'load_checkpoint': None}, 'optimizer': {'weight_decay': 5e-05, 'learning_rate': 2e-05, 'eps': 1e-08, 'betas': [0.9, 0.999]}, 'scheduler': {'scheduler_type': 'linear_warmup_cosine_annealing_lr', 'batch_scheduler': True, 'constant_schedule_with_warmup': {'n_warmup_steps': 0}, 'linear_schedule_with_warmup': {'n_warmup_steps': 0}, 'cosine_schedule_with_warmup': {'n_cycles': 0.5, 'n_warmup_steps': 0}, 'polynomial_decay_schedule_with_warmup': {'n_warmup_steps': 0, 'power': 1.0, 'min_lr': 0.0}, 'linear_warmup_cosine_annealing_lr': {'warmup_epochs': 0, 'max_epochs': 20}}, 'arch': {'base_settings': {'video_params': {'model': 'SpaceTimeTransformer', 'arch_config': 'base_patch16_224', 'num_frames': 4, 'pretrained': True, 'time_init': 'zeros', 'input_res': 224, 'color_classes': 8, 'type_classes': 6, 'motion_classes': 4}, 'text_params': {'model': 'distilbert-base-uncased', 'pretrained': True, 'input': 'text'}, 'projection': 'minimal', 'load_checkpoint': './checkpoint/archive/cc-webvid2m-4f_stformer_b_16_224.pth.tar'}, 'text_head_setting': {'type': 'ContextualizedWeightedHead', 'args': {'d_model': 256, 'nhead': 8, 'num_layers': 2, 'fc_dim_list': [256, 512, 1]}}, 'vision_head_setting': {'motion_encoder': 'vgg16', 'motion_line_encoder': 'vgg16', 'type': 'ContextualizedWeightedHead', 'args': {'d_model': 256, 'nhead': 8, 'num_layers': 2, 'fc_dim_list': [256, 512, 1]}}}}\n"
     ]
    }
   ],
   "source": [
    "from dotwiz import DotWiz\n",
    "with open('./configs/baseline_config.json', \"r\") as f:\n",
    "    config = json.load(f)\n",
    "print(config)\n",
    "\n",
    "config = DotWiz(config)\n",
    "\n",
    "config['general_config']['tokenizer'] = tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "config.general_config.train_batch_size = 2\n",
    "config.general_config.valid_batch_size = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1524\n",
       "2     244\n",
       "1     213\n",
       "3     174\n",
       "Name: motion, dtype: int64"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df['motion'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = get_train_dataloader(config, data_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "2\n",
      "1\n",
      "3\n",
      "2\n",
      "0\n",
      "32\n",
      "\n",
      "2\n",
      "2\n",
      "0\n",
      "3\n",
      "1\n",
      "1\n",
      "1\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "batch = next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "from loss.crossentropy import CrossEntropyLabelSmooth\n",
    "\n",
    "criterion = CrossEntropyLabelSmooth()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = torch.rand(4, 6)\n",
    "label = torch.randint(6, size=(4,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.8436)"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "criterion(inputs, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1], dtype=torch.int32)"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.IntTensor([1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_1121_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8569045e15b11543999fc62c48593176458b9767f19dc62a9c1d5aa1e4e3600f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
