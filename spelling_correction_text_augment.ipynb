{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "nLqPjCK0ZEhS",
        "3-xctlUFwW6z",
        "agkD59TdRhBR",
        "YjL6fxE3Ry8f"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Using symspellpy - spell check\n",
        "\n",
        "[github](https://github.com/mammothb/symspellpy/)"
      ],
      "metadata": {
        "id": "nLqPjCK0ZEhS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m pip install -U symspellpy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pwZrwxNvWgNW",
        "outputId": "cc52c23b-d124-440d-a20f-0f7c195ff7ab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting symspellpy\n",
            "  Downloading symspellpy-6.7.7-py3-none-any.whl (2.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.6 MB 5.0 MB/s \n",
            "\u001b[?25hCollecting editdistpy>=0.1.3\n",
            "  Downloading editdistpy-0.1.3-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (126 kB)\n",
            "\u001b[K     |████████████████████████████████| 126 kB 67.3 MB/s \n",
            "\u001b[?25hInstalling collected packages: editdistpy, symspellpy\n",
            "Successfully installed editdistpy-0.1.3 symspellpy-6.7.7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Alternatively, you can download the dictionary files from the repository and add them to your project directory:\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "curl -LJO https://raw.githubusercontent.com/mammothb/symspellpy/master/symspellpy/frequency_dictionary_en_82_765.txt\n",
        "curl -LJO https://raw.githubusercontent.com/mammothb/symspellpy/master/symspellpy/frequency_bigramdictionary_en_243_342.txt\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "Svtmy_BLZbez"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Spell check"
      ],
      "metadata": {
        "id": "vctA_QqeZSe9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from itertools import islice\n",
        "import pkg_resources\n",
        "from symspellpy import SymSpell\n",
        "\n",
        "input_term = \"thequickbrownfoxjumpsoverthelazydog\"\n",
        "\n",
        "def spelling_correction_en(input_term):\n",
        "  \n",
        "  sym_spell = SymSpell()\n",
        "\n",
        "  dictionary_path = pkg_resources.resource_filename(\"symspellpy\", \"frequency_dictionary_en_82_765.txt\")\n",
        "  bigram_path = pkg_resources.resource_filename(\n",
        "      \"symspellpy\", \"frequency_bigramdictionary_en_243_342.txt\"\n",
        "  )\n",
        "  # term_index is the column of the term and count_index is theolumn of the term frequency\n",
        "  sym_spell.load_dictionary(dictionary_path, term_index=0, count_index=1)\n",
        "  sym_spell.load_bigram_dictionary(bigram_path, term_index=0, count_index=2)\n",
        "  \n",
        "  # result\n",
        "  result = sym_spell.word_segmentation(input_term)\n",
        "  # print(result.corrected_string)\n",
        "  return (result.corrected_string).lower()"
      ],
      "metadata": {
        "id": "3H1t80rhWgPZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# test = \"whereis th elove hehad dated forImuch of thepast whocouqdn'tread in sixtgrade and ins pired him\"\n",
        "test = [\n",
        "\"A red sedan drives forward.\",\n",
        "      \"A red midsize sedan keep straight.\",\n",
        "      \"A red car drove through an intersection.\"\n",
        "]\n",
        "result = spelling_correction_en(test[0])\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mgL1AHp_gDff",
        "outputId": "6782bcc3-5462-4c88-8029-f62b03f50bfe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "a red sedan drives forward\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## NLTK"
      ],
      "metadata": {
        "id": "3-xctlUFwW6z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/dwyl/english-words/master/words.txt\n",
        "!wget http://norvig.com/big.txt"
      ],
      "metadata": {
        "id": "RCnudrQkXjlp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f47d3ff8-9178-42ca-ef83-3a7f3343f692"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-12-09 06:38:58--  https://raw.githubusercontent.com/dwyl/english-words/master/words.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.111.133, 185.199.108.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4862992 (4.6M) [text/plain]\n",
            "Saving to: ‘words.txt’\n",
            "\n",
            "words.txt           100%[===================>]   4.64M  --.-KB/s    in 0.08s   \n",
            "\n",
            "2022-12-09 06:38:59 (57.3 MB/s) - ‘words.txt’ saved [4862992/4862992]\n",
            "\n",
            "--2022-12-09 06:38:59--  http://norvig.com/big.txt\n",
            "Resolving norvig.com (norvig.com)... 158.106.138.13\n",
            "Connecting to norvig.com (norvig.com)|158.106.138.13|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 6488666 (6.2M) [text/plain]\n",
            "Saving to: ‘big.txt’\n",
            "\n",
            "big.txt             100%[===================>]   6.19M  31.7MB/s    in 0.2s    \n",
            "\n",
            "2022-12-09 06:38:59 (31.7 MB/s) - ‘big.txt’ saved [6488666/6488666]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from collections import Counter\n",
        "\n",
        "TEXT = open('/content/big.txt').read()\n",
        "alphabet = 'abcdefghijklmnopqrstuvwxyz'\n",
        "\n",
        "def tokens(text):\n",
        "    \"List all the word tokens (consecutive letters) in a text. Normalize to lowercase.\"\n",
        "    return re.findall('[a-z]+', text.lower()) \n",
        "  \n",
        "WORDS = tokens(TEXT)\n",
        "COUNTS = Counter(WORDS)\n",
        "\n",
        "def known(words):\n",
        "    \"Return the subset of words that are actually in the dictionary.\"\n",
        "    return {w for w in words if w in COUNTS}\n",
        "\n",
        "def edits0(word): \n",
        "    \"Return all strings that are zero edits away from word (i.e., just word itself).\"\n",
        "    return {word}\n",
        "\n",
        "def edits1(word):\n",
        "    \"Return all strings that are one edit away from this word.\"\n",
        "    pairs      = splits(word)\n",
        "    deletes    = [a+b[1:]           for (a, b) in pairs if b]\n",
        "    transposes = [a+b[1]+b[0]+b[2:] for (a, b) in pairs if len(b) > 1]\n",
        "    replaces   = [a+c+b[1:]         for (a, b) in pairs for c in alphabet if b]\n",
        "    inserts    = [a+c+b             for (a, b) in pairs for c in alphabet]\n",
        "    return set(deletes + transposes + replaces + inserts)\n",
        "\n",
        "def edits2(word):\n",
        "    \"Return all strings that are two edits away from this word.\"\n",
        "    return {e2 for e1 in edits1(word) for e2 in edits1(e1)}\n",
        "\n",
        "def correct(word):\n",
        "    \"Find the best spelling correction for this word.\"\n",
        "    # Prefer edit distance 0, then 1, then 2; otherwise default to word itself.\n",
        "    candidates = (known(edits0(word)) or \n",
        "                  known(edits1(word)) or \n",
        "                  known(edits2(word)) or \n",
        "                  [word])\n",
        "    return max(candidates, key=COUNTS.get)\n",
        "\n",
        "def splits(word):\n",
        "    \"Return a list of all possible (first, rest) pairs that comprise word.\"\n",
        "    return [(word[:i], word[i:]) \n",
        "            for i in range(len(word)+1)]\n",
        "\n",
        "def correct_text(text):\n",
        "    \"Correct all the words within a text, returning the corrected text.\"\n",
        "    return re.sub('[a-zA-Z]+', correct_match, text)\n",
        "\n",
        "def case_of(text):\n",
        "    \"Return the case-function appropriate for text: upper, lower, title, or just str.\"\n",
        "    return (str.upper if text.isupper() else\n",
        "            str.lower if text.islower() else\n",
        "            str.title if text.istitle() else\n",
        "            str)\n",
        "\n",
        "def correct_match(match):\n",
        "    \"Spell-correct word in match, and preserve proper upper/lower/title case.\"\n",
        "    word = match.group()\n",
        "    return case_of(word)(correct(word.lower()))"
      ],
      "metadata": {
        "id": "fhWUrMqhwWvy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Flair (Semantic role labeling) \n"
      ],
      "metadata": {
        "id": "wJXo74dx7Cgs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install flair"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IwHnVcoo8Swa",
        "outputId": "7baab015-b21b-4028-fa2a-68151177e980"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: flair in /usr/local/lib/python3.8/dist-packages (0.11.3)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.8/dist-packages (from flair) (4.9.1)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.8/dist-packages (from flair) (2022.6.2)\n",
            "Requirement already satisfied: sqlitedict>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from flair) (2.1.0)\n",
            "Requirement already satisfied: deprecated>=1.2.4 in /usr/local/lib/python3.8/dist-packages (from flair) (1.2.13)\n",
            "Requirement already satisfied: transformers>=4.0.0 in /usr/local/lib/python3.8/dist-packages (from flair) (4.25.1)\n",
            "Requirement already satisfied: gensim>=3.4.0 in /usr/local/lib/python3.8/dist-packages (from flair) (3.6.0)\n",
            "Requirement already satisfied: pptree in /usr/local/lib/python3.8/dist-packages (from flair) (3.1)\n",
            "Requirement already satisfied: sentencepiece==0.1.95 in /usr/local/lib/python3.8/dist-packages (from flair) (0.1.95)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.8/dist-packages (from flair) (0.11.1)\n",
            "Requirement already satisfied: langdetect in /usr/local/lib/python3.8/dist-packages (from flair) (1.0.9)\n",
            "Requirement already satisfied: ftfy in /usr/local/lib/python3.8/dist-packages (from flair) (6.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.8/dist-packages (from flair) (2.8.2)\n",
            "Requirement already satisfied: tqdm>=4.26.0 in /usr/local/lib/python3.8/dist-packages (from flair) (4.64.1)\n",
            "Requirement already satisfied: hyperopt>=0.2.7 in /usr/local/lib/python3.8/dist-packages (from flair) (0.2.7)\n",
            "Requirement already satisfied: matplotlib>=2.2.3 in /usr/local/lib/python3.8/dist-packages (from flair) (3.2.2)\n",
            "Requirement already satisfied: segtok>=1.5.7 in /usr/local/lib/python3.8/dist-packages (from flair) (1.5.11)\n",
            "Requirement already satisfied: konoha<5.0.0,>=4.0.0 in /usr/local/lib/python3.8/dist-packages (from flair) (4.6.5)\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.8/dist-packages (from flair) (9.0.0)\n",
            "Requirement already satisfied: mpld3==0.3 in /usr/local/lib/python3.8/dist-packages (from flair) (0.3)\n",
            "Requirement already satisfied: conllu>=4.0 in /usr/local/lib/python3.8/dist-packages (from flair) (4.5.2)\n",
            "Requirement already satisfied: wikipedia-api in /usr/local/lib/python3.8/dist-packages (from flair) (0.5.4)\n",
            "Requirement already satisfied: torch!=1.8,>=1.5.0 in /usr/local/lib/python3.8/dist-packages (from flair) (1.13.0+cu116)\n",
            "Requirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.8/dist-packages (from flair) (1.0.2)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.8/dist-packages (from flair) (0.8.10)\n",
            "Requirement already satisfied: gdown==4.4.0 in /usr/local/lib/python3.8/dist-packages (from flair) (4.4.0)\n",
            "Requirement already satisfied: bpemb>=0.3.2 in /usr/local/lib/python3.8/dist-packages (from flair) (0.3.4)\n",
            "Requirement already satisfied: janome in /usr/local/lib/python3.8/dist-packages (from flair) (0.4.2)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.8/dist-packages (from gdown==4.4.0->flair) (2.28.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from gdown==4.4.0->flair) (3.8.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.8/dist-packages (from gdown==4.4.0->flair) (4.6.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.8/dist-packages (from gdown==4.4.0->flair) (1.15.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from bpemb>=0.3.2->flair) (1.21.6)\n",
            "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.8/dist-packages (from deprecated>=1.2.4->flair) (1.14.1)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.8/dist-packages (from gensim>=3.4.0->flair) (1.7.3)\n",
            "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.8/dist-packages (from gensim>=3.4.0->flair) (5.2.1)\n",
            "Requirement already satisfied: py4j in /usr/local/lib/python3.8/dist-packages (from hyperopt>=0.2.7->flair) (0.10.9.7)\n",
            "Requirement already satisfied: networkx>=2.2 in /usr/local/lib/python3.8/dist-packages (from hyperopt>=0.2.7->flair) (2.8.8)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.8/dist-packages (from hyperopt>=0.2.7->flair) (0.16.0)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.8/dist-packages (from hyperopt>=0.2.7->flair) (1.5.0)\n",
            "Requirement already satisfied: importlib-metadata<4.0.0,>=3.7.0 in /usr/local/lib/python3.8/dist-packages (from konoha<5.0.0,>=4.0.0->flair) (3.10.1)\n",
            "Requirement already satisfied: overrides<4.0.0,>=3.0.0 in /usr/local/lib/python3.8/dist-packages (from konoha<5.0.0,>=4.0.0->flair) (3.1.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata<4.0.0,>=3.7.0->konoha<5.0.0,>=4.0.0->flair) (3.11.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib>=2.2.3->flair) (1.4.4)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib>=2.2.3->flair) (3.0.9)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.8/dist-packages (from matplotlib>=2.2.3->flair) (0.11.0)\n",
            "Requirement already satisfied: charset-normalizer<3,>=2 in /usr/local/lib/python3.8/dist-packages (from requests[socks]->gdown==4.4.0->flair) (2.1.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests[socks]->gdown==4.4.0->flair) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests[socks]->gdown==4.4.0->flair) (2022.9.24)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests[socks]->gdown==4.4.0->flair) (1.24.3)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.8/dist-packages (from scikit-learn>=0.21.3->flair) (1.2.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn>=0.21.3->flair) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch!=1.8,>=1.5.0->flair) (4.4.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from transformers>=4.0.0->flair) (6.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.8/dist-packages (from transformers>=4.0.0->flair) (0.13.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from transformers>=4.0.0->flair) (21.3)\n",
            "Requirement already satisfied: wcwidth>=0.2.5 in /usr/local/lib/python3.8/dist-packages (from ftfy->flair) (0.2.5)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.8/dist-packages (from requests[socks]->gdown==4.4.0->flair) (1.7.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tagging Text"
      ],
      "metadata": {
        "id": "agkD59TdRhBR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from flair.models import SequenceTagger\n",
        "from flair.data import Sentence\n",
        "\n",
        "# load model\n",
        "tagger = SequenceTagger.load('upos-fast')\n",
        "\n",
        "# text with English and German sentences\n",
        "# sentence = Sentence('George Washington went')\n",
        "sentence = Sentence(result)\n",
        "\n",
        "# predict PoS tags\n",
        "tagger.predict(sentence)\n",
        "\n",
        "# print sentence with predicted tags\n",
        "print(sentence)\n",
        "\n",
        "for label in sentence.get_labels('pos'):\n",
        "  # print(label.text)\n",
        "  print(label.data_point.text)\n",
        "  print(label.value)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SWSXOTYSIE89",
        "outputId": "ec731476-9370-4bb5-8ebe-822bf98fa42b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2022-12-09 06:39:04,972 loading file /root/.flair/models/upos-english-fast/b631371788604e95f27b6567fe7220e4a7e8d03201f3d862e6204dbf90f9f164.0afb95b43b32509bf4fcc3687f7c64157d8880d08f813124c1bd371c3d8ee3f7\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/huggingface_hub/file_download.py:594: FutureWarning: `cached_download` is the legacy way to download files from the HF hub, please consider upgrading to `hf_hub_download`\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2022-12-09 06:39:05,062 SequenceTagger predicts: Dictionary with 20 tags: <unk>, O, INTJ, PUNCT, VERB, PRON, NOUN, ADV, DET, ADJ, ADP, NUM, PROPN, CCONJ, PART, AUX, X, SYM, <START>, <STOP>\n",
            "Sentence: \"a red sedan drives forward\" → [\"a\"/DET, \"red\"/ADJ, \"sedan\"/NOUN, \"drives\"/VERB, \"forward\"/ADV]\n",
            "a\n",
            "DET\n",
            "red\n",
            "ADJ\n",
            "sedan\n",
            "NOUN\n",
            "drives\n",
            "VERB\n",
            "forward\n",
            "ADV\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Use Case 2: Zero-shot Named Entity Recognition (NER) with TARS"
      ],
      "metadata": {
        "id": "rzDF94z4Rpr5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from flair.models import TARSTagger\n",
        "from flair.data import Sentence\n",
        "\n",
        "# 1. Load zero-shot NER tagger\n",
        "tars = TARSTagger.load('tars-ner')\n",
        "\n",
        "# 2. Prepare some test sentences\n",
        "sentences = [\n",
        "    Sentence(\"A red midsize sedan keep straight.\"),\n",
        "    Sentence(\"A red sedan drives forward.\"),\n",
        "    Sentence(\"A red sedan keeping straight.\"),\n",
        "]\n",
        "\n",
        "# 3. Define some classes of named entities such as \"color\", \"type\", \"motion\"\n",
        "labels = [\"color\", \"type\", \"motion\"]\n",
        "tars.add_and_switch_to_new_task('task 1', labels, label_type='pos')\n",
        "\n",
        "# 4. Predict for these classes and print results\n",
        "for sentence in sentences:\n",
        "    tars.predict(sentence)\n",
        "    print(sentence.to_tagged_string(\"ner\"))"
      ],
      "metadata": {
        "id": "na4WLHH3ILD7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## word count nlp"
      ],
      "metadata": {
        "id": "oxShFEc8XFAd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk import *\n",
        "from nltk.tokenize import sent_tokenize,word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "id": "_9Cx9kCtXR8D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dict_w = []\n",
        "s = ['A', 'red', 'sedan', 'drives', 'forward', '.']\n",
        "print(type(s))\n",
        "dict_w = dict_w + s\n",
        "dict_w"
      ],
      "metadata": {
        "id": "wEMRnwHrboI9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample=[\n",
        "    \"A red midsize sedan keep straight.\",\n",
        "    \"A red sedan drives forward.\",\n",
        "    \"A red sedan keeping straight.\",\n",
        "]\n",
        "words = []\n",
        "for s in sample:\n",
        "  sent=(sent_tokenize(s))        #splitting sentence\n",
        "  word=(word_tokenize(s))        #splitting words\n",
        "\n",
        "  words += word\n",
        "\n",
        "  # print(sent)\n",
        "  print(word)\n",
        "\n",
        "stop_word=set(stopwords.words(\"english\"))       #depicts all stopwords fot english language\n",
        "new=[]\n",
        "for i in words:\n",
        "    if i not in stop_word:\n",
        "        new.append(i)\n",
        "print(new)\n",
        "\n",
        "tot_sent=FreqDist(sent)     #total sentences length\n",
        "count=len(tot_sent)\n",
        "print(count)\n",
        "tot_word=FreqDist(new)     #total words length\n",
        "counts=len(tot_word)\n",
        "print(counts)\n",
        "print(tot_word.most_common(25))     #how much times a word repeat"
      ],
      "metadata": {
        "id": "cBBWckRyXIb1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fJsX0KF9dw6E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Spacy + Pattern -> past tense "
      ],
      "metadata": {
        "id": "YjL6fxE3Ry8f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install spacy\n",
        "!pip install pattern"
      ],
      "metadata": {
        "id": "i5D4jiX0KVI2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('omw-1.4')"
      ],
      "metadata": {
        "id": "ABySlHs2SnlD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "import spacy\n",
        "from spacy.lang.en import English\n",
        "from spacy.symbols import nsubj, VERB, NOUN\n",
        "from pattern.en import conjugate, PAST, PRESENT, tenses, parse, pprint, parsetree, SINGULAR, PLURAL\n",
        "from itertools import tee\n",
        "import string\n",
        "from html.parser import HTMLParser"
      ],
      "metadata": {
        "id": "iGQOGAAPSEnN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SUBJ_DEPS = {'agent', 'csubj', 'csubjpass', 'expl', 'nsubj', 'nsubjpass'}\n",
        "\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "def _get_conjuncts(tok):\n",
        "    \"\"\"\n",
        "    Return conjunct dependents of the leftmost conjunct in a coordinated phrase,\n",
        "    e.g. \"Burton, [Dan], and [Josh] ...\".\n",
        "    \"\"\"\n",
        "    return [right for right in tok.rights\n",
        "            if right.dep_ == 'conj']\n",
        "\n",
        "\n",
        "def is_plural_noun(token):\n",
        "    \"\"\"\n",
        "    Returns True if token is a plural noun, False otherwise.\n",
        "\n",
        "    Args:\n",
        "        token (``spacy.Token``): parent document must have POS information\n",
        "\n",
        "    Returns:\n",
        "        bool\n",
        "    \"\"\"\n",
        "    if token.doc.is_tagged is False:\n",
        "        raise ValueError('token is not POS-tagged')\n",
        "    return True if token.pos == NOUN and token.lemma != token.lower else False\n",
        "\n",
        "\n",
        "def get_subjects_of_verb(verb):\n",
        "    if verb.dep_ == \"aux\" and list(verb.ancestors):\n",
        "        return get_subjects_of_verb(list(verb.ancestors)[0])\n",
        "    \"\"\"Return all subjects of a verb according to the dependency parse.\"\"\"\n",
        "    subjs = [tok for tok in verb.lefts\n",
        "             if tok.dep_ in SUBJ_DEPS]\n",
        "    # get additional conjunct subjects\n",
        "    subjs.extend(tok for subj in subjs for tok in _get_conjuncts(subj))\n",
        "    if not len(subjs):\n",
        "        ancestors = list(verb.ancestors)\n",
        "        if len(ancestors) > 0:\n",
        "            return get_subjects_of_verb(ancestors[0])\n",
        "    return subjs\n",
        "\n",
        "\n",
        "def is_plural_verb(token):\n",
        "    if token.doc.is_tagged is False:\n",
        "        raise ValueError('token is not POS-tagged')\n",
        "    subjects = get_subjects_of_verb(token)\n",
        "    if not len(subjects):\n",
        "        return False\n",
        "    plural_score = sum([is_plural_noun(x) for x in subjects])/len(subjects)\n",
        "\n",
        "    return plural_score > .5\n",
        "\n",
        "def preserve_caps(word, newWord):\n",
        "    \"\"\"Returns newWord, capitalizing it if word is capitalized.\"\"\"\n",
        "    if word[0] >= 'A' and word[0] <= 'Z':\n",
        "        newWord = newWord.capitalize()\n",
        "    return newWord\n",
        "\n",
        "def change_tense(text, to_tense, nlp=nlp):\n",
        "    \"\"\"Change the tense of text.\n",
        "\n",
        "    Args:\n",
        "        text (str): text to change.\n",
        "        to_tense (str): 'present','past', or 'future'\n",
        "        npl (SpaCy model, optional):\n",
        "\n",
        "    Returns:\n",
        "        str: changed text.\n",
        "\n",
        "    \"\"\"\n",
        "    tense_lookup = {'future': 'inf', 'present': PRESENT, 'past': PAST}\n",
        "    tense = tense_lookup[to_tense]\n",
        "\n",
        "    doc = nlp(text)\n",
        "\n",
        "    out = list()\n",
        "    out.append(doc[0].text)\n",
        "    words = []\n",
        "    for word in doc:\n",
        "        words.append(word)\n",
        "        if len(words) == 1:\n",
        "            continue\n",
        "        if (words[-2].text == 'will' and words[-2].tag_ == 'MD' and words[-1].tag_ == 'VB') or \\\n",
        "                        words[-1].tag_ in ('VBD', 'VBP', 'VBZ', 'VBN') or \\\n",
        "                (not words[-2].text in ('to', 'not') and words[-1].tag_ == 'VB'):\n",
        "\n",
        "            if words[-2].text in ('were', 'am', 'is', 'are', 'was') or\\\n",
        "                    (words[-2].text == 'be' and len(words) > 2 and words[-3].text == 'will'):\n",
        "                this_tense = tense_lookup['past']\n",
        "            else:\n",
        "                this_tense = tense\n",
        "\n",
        "            subjects = [x.text for x in get_subjects_of_verb(words[-1])]\n",
        "            if ('I' in subjects) or ('we' in subjects) or ('We' in subjects):\n",
        "                person = 1\n",
        "            elif ('you' in subjects) or ('You' in subjects):\n",
        "                person = 2\n",
        "            else:\n",
        "                person = 3\n",
        "            if is_plural_verb(words[-1]):\n",
        "                number = PLURAL\n",
        "            else:\n",
        "                number = SINGULAR\n",
        "            if (words[-2].text == 'will' and words[-2].tag_ == 'MD') or words[-2].text == 'had':\n",
        "                out.pop(-1)\n",
        "            if to_tense == 'future':\n",
        "                if not (out[-1] == 'will' or out[-1] == 'be'):\n",
        "                    out.append('will')\n",
        "                # handle will as a noun in future tense\n",
        "                if words[-2].text == 'will' and words[-2].tag_ == 'NN':\n",
        "                    out.append('will')\n",
        "            #if word_pair[0].dep_ == 'auxpass':\n",
        "            oldWord = words[-1].text\n",
        "            out.append(preserve_caps(oldWord, conjugate(oldWord, tense=this_tense, person=person, number=number)))\n",
        "        else:\n",
        "            out.append(words[-1].text)\n",
        "\n",
        "        # negation\n",
        "        if words[-2].text + words[-1].text in ('didnot', 'donot', 'willnot', \"didn't\", \"don't\", \"won't\"):\n",
        "            if tense == PAST:\n",
        "                out[-2] = 'did'\n",
        "            elif tense == PRESENT:\n",
        "                out[-2] = 'do'\n",
        "            else:\n",
        "                out.pop(-2)\n",
        "\n",
        "        # future perfect, and progressives, but ignore for \"I will have cookies\"\n",
        "        if words[-1].text in ('have', 'has') and len(list(words[-1].ancestors)) and words[-1].dep_ == 'aux':\n",
        "            out.pop(-1)\n",
        "\n",
        "    text_out = ' '.join(out)\n",
        "\n",
        "    # Remove spaces before/after punctuation:\n",
        "    for char in string.punctuation:\n",
        "        if char in \"\"\"(<['\"\"\":\n",
        "            text_out = text_out.replace(char+' ', char)\n",
        "        else:\n",
        "            text_out = text_out.replace(' '+char, char)\n",
        "\n",
        "    for char in [\"-\", \"“\", \"‘\"]:\n",
        "        text_out = text_out.replace(char+' ', char)\n",
        "    for char in [\"…\", \"”\", \"'s\", \"n't\"]:\n",
        "        text_out = text_out.replace(' '+char, char)\n",
        "\n",
        "    return text_out"
      ],
      "metadata": {
        "id": "ulJRyjETSJJg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # fix bug\n",
        "# try:\n",
        "#     yield line\n",
        "# except StopIteration:\n",
        "#     return"
      ],
      "metadata": {
        "id": "a-86Eu3TUzaM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test = 'It is a awesome weekend Sitting at the windows I can see bird chirping'\n",
        "r = change_tense(test, 'past')"
      ],
      "metadata": {
        "id": "_XECqUfZS2jZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "r"
      ],
      "metadata": {
        "id": "ftjGXVbeTssD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Text augmentation"
      ],
      "metadata": {
        "id": "rJwMBqrNZJ8C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "id": "mQU6mDBvZYPG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Easy data augmentation techniques for text classification\n",
        "# Jason Wei and Kai Zou\n",
        "\n",
        "import random\n",
        "from random import shuffle\n",
        "random.seed(1)\n",
        "\n",
        "#stop words list\n",
        "stop_words = ['i', 'me', 'my', 'myself', 'we', 'our', \n",
        "\t\t\t'ours', 'ourselves', 'you', 'your', 'yours', \n",
        "\t\t\t'yourself', 'yourselves', 'he', 'him', 'his', \n",
        "\t\t\t'himself', 'she', 'her', 'hers', 'herself', \n",
        "\t\t\t'it', 'its', 'itself', 'they', 'them', 'their', \n",
        "\t\t\t'theirs', 'themselves', 'what', 'which', 'who', \n",
        "\t\t\t'whom', 'this', 'that', 'these', 'those', 'am', \n",
        "\t\t\t'is', 'are', 'was', 'were', 'be', 'been', 'being', \n",
        "\t\t\t'have', 'has', 'had', 'having', 'do', 'does', 'did',\n",
        "\t\t\t'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or',\n",
        "\t\t\t'because', 'as', 'until', 'while', 'of', 'at', \n",
        "\t\t\t'by', 'for', 'with', 'about', 'against', 'between',\n",
        "\t\t\t'into', 'through', 'during', 'before', 'after', \n",
        "\t\t\t'above', 'below', 'to', 'from', 'up', 'down', 'in',\n",
        "\t\t\t'out', 'on', 'off', 'over', 'under', 'again', \n",
        "\t\t\t'further', 'then', 'once', 'here', 'there', 'when', \n",
        "\t\t\t'where', 'why', 'how', 'all', 'any', 'both', 'each', \n",
        "\t\t\t'few', 'more', 'most', 'other', 'some', 'such', 'no', \n",
        "\t\t\t'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', \n",
        "\t\t\t'very', 's', 't', 'can', 'will', 'just', 'don', \n",
        "\t\t\t'should', 'now', '']\n",
        "\n",
        "#cleaning up text\n",
        "import re\n",
        "def get_only_chars(line):\n",
        "\n",
        "    clean_line = \"\"\n",
        "\n",
        "    line = line.replace(\"’\", \"\")\n",
        "    line = line.replace(\"'\", \"\")\n",
        "    line = line.replace(\"-\", \" \") #replace hyphens with spaces\n",
        "    line = line.replace(\"\\t\", \" \")\n",
        "    line = line.replace(\"\\n\", \" \")\n",
        "    line = line.lower()\n",
        "\n",
        "    for char in line:\n",
        "        if char in 'qwertyuiopasdfghjklzxcvbnm ':\n",
        "            clean_line += char\n",
        "        else:\n",
        "            clean_line += ' '\n",
        "\n",
        "    clean_line = re.sub(' +',' ',clean_line) #delete extra spaces\n",
        "    if clean_line[0] == ' ':\n",
        "        clean_line = clean_line[1:]\n",
        "    return clean_line\n",
        "\n",
        "########################################################################\n",
        "# Synonym replacement\n",
        "# Replace n words in the sentence with synonyms from wordnet\n",
        "########################################################################\n",
        "\n",
        "#for the first time you use wordnet\n",
        "#import nltk\n",
        "#nltk.download('wordnet')\n",
        "from nltk.corpus import wordnet \n",
        "\n",
        "def synonym_replacement(words, n):\n",
        "\tnew_words = words.copy()\n",
        "\trandom_word_list = list(set([word for word in words if word not in stop_words]))\n",
        "\trandom.shuffle(random_word_list)\n",
        "\tnum_replaced = 0\n",
        "\tfor random_word in random_word_list:\n",
        "\t\tsynonyms = get_synonyms(random_word)\n",
        "\t\tif len(synonyms) >= 1:\n",
        "\t\t\tsynonym = random.choice(list(synonyms))\n",
        "\t\t\tnew_words = [synonym if word == random_word else word for word in new_words]\n",
        "\t\t\t#print(\"replaced\", random_word, \"with\", synonym)\n",
        "\t\t\tnum_replaced += 1\n",
        "\t\tif num_replaced >= n: #only replace up to n words\n",
        "\t\t\tbreak\n",
        "\n",
        "\t#this is stupid but we need it, trust me\n",
        "\tsentence = ' '.join(new_words)\n",
        "\tnew_words = sentence.split(' ')\n",
        "\n",
        "\treturn new_words\n",
        "\n",
        "def get_synonyms(word):\n",
        "\tsynonyms = set()\n",
        "\tfor syn in wordnet.synsets(word): \n",
        "\t\tfor l in syn.lemmas(): \n",
        "\t\t\tsynonym = l.name().replace(\"_\", \" \").replace(\"-\", \" \").lower()\n",
        "\t\t\tsynonym = \"\".join([char for char in synonym if char in ' qwertyuiopasdfghjklzxcvbnm'])\n",
        "\t\t\tsynonyms.add(synonym) \n",
        "\tif word in synonyms:\n",
        "\t\tsynonyms.remove(word)\n",
        "\treturn list(synonyms)\n",
        "\n",
        "########################################################################\n",
        "# Random deletion\n",
        "# Randomly delete words from the sentence with probability p\n",
        "########################################################################\n",
        "\n",
        "def random_deletion(words, p):\n",
        "\n",
        "\t#obviously, if there's only one word, don't delete it\n",
        "\tif len(words) == 1:\n",
        "\t\treturn words\n",
        "\n",
        "\t#randomly delete words with probability p\n",
        "\tnew_words = []\n",
        "\tfor word in words:\n",
        "\t\tr = random.uniform(0, 1)\n",
        "\t\tif r > p:\n",
        "\t\t\tnew_words.append(word)\n",
        "\n",
        "\t#if you end up deleting all words, just return a random word\n",
        "\tif len(new_words) == 0:\n",
        "\t\trand_int = random.randint(0, len(words)-1)\n",
        "\t\treturn [words[rand_int]]\n",
        "\n",
        "\treturn new_words\n",
        "\n",
        "########################################################################\n",
        "# Random swap\n",
        "# Randomly swap two words in the sentence n times\n",
        "########################################################################\n",
        "\n",
        "def random_swap(words, n):\n",
        "\tnew_words = words.copy()\n",
        "\tfor _ in range(n):\n",
        "\t\tnew_words = swap_word(new_words)\n",
        "\treturn new_words\n",
        "\n",
        "def swap_word(new_words):\n",
        "\trandom_idx_1 = random.randint(0, len(new_words)-1)\n",
        "\trandom_idx_2 = random_idx_1\n",
        "\tcounter = 0\n",
        "\twhile random_idx_2 == random_idx_1:\n",
        "\t\trandom_idx_2 = random.randint(0, len(new_words)-1)\n",
        "\t\tcounter += 1\n",
        "\t\tif counter > 3:\n",
        "\t\t\treturn new_words\n",
        "\tnew_words[random_idx_1], new_words[random_idx_2] = new_words[random_idx_2], new_words[random_idx_1] \n",
        "\treturn new_words\n",
        "\n",
        "########################################################################\n",
        "# Random insertion\n",
        "# Randomly insert n words into the sentence\n",
        "########################################################################\n",
        "\n",
        "def random_insertion(words, n):\n",
        "\tnew_words = words.copy()\n",
        "\tfor _ in range(n):\n",
        "\t\tadd_word(new_words)\n",
        "\treturn new_words\n",
        "\n",
        "def add_word(new_words):\n",
        "\tsynonyms = []\n",
        "\tcounter = 0\n",
        "\twhile len(synonyms) < 1:\n",
        "\t\trandom_word = new_words[random.randint(0, len(new_words)-1)]\n",
        "\t\tsynonyms = get_synonyms(random_word)\n",
        "\t\tcounter += 1\n",
        "\t\tif counter >= 10:\n",
        "\t\t\treturn\n",
        "\trandom_synonym = synonyms[0]\n",
        "\trandom_idx = random.randint(0, len(new_words)-1)\n",
        "\tnew_words.insert(random_idx, random_synonym)\n",
        "\n",
        "########################################################################\n",
        "# main data augmentation function\n",
        "########################################################################\n",
        "\n",
        "def eda(sentence, alpha_sr=0.1, alpha_ri=0.1, alpha_rs=0.1, p_rd=0.1, num_aug=9):\n",
        "\t\n",
        "\tsentence = get_only_chars(sentence)\n",
        "\twords = sentence.split(' ')\n",
        "\twords = [word for word in words if word != '']\n",
        "\tnum_words = len(words)\n",
        "\t\n",
        "\taugmented_sentences = []\n",
        "\tnum_new_per_technique = int(num_aug/4)+1\n",
        "\n",
        "\t#sr\n",
        "\tif (alpha_sr > 0):\n",
        "\t\tn_sr = max(1, int(alpha_sr*num_words))\n",
        "\t\tfor _ in range(num_new_per_technique):\n",
        "\t\t\ta_words = synonym_replacement(words, n_sr)\n",
        "\t\t\taugmented_sentences.append(' '.join(a_words))\n",
        "\n",
        "\t#ri\n",
        "\tif (alpha_ri > 0):\n",
        "\t\tn_ri = max(1, int(alpha_ri*num_words))\n",
        "\t\tfor _ in range(num_new_per_technique):\n",
        "\t\t\ta_words = random_insertion(words, n_ri)\n",
        "\t\t\taugmented_sentences.append(' '.join(a_words))\n",
        "\n",
        "\t#rs\n",
        "\tif (alpha_rs > 0):\n",
        "\t\tn_rs = max(1, int(alpha_rs*num_words))\n",
        "\t\tfor _ in range(num_new_per_technique):\n",
        "\t\t\ta_words = random_swap(words, n_rs)\n",
        "\t\t\taugmented_sentences.append(' '.join(a_words))\n",
        "\n",
        "\t#rd\n",
        "\tif (p_rd > 0):\n",
        "\t\tfor _ in range(num_new_per_technique):\n",
        "\t\t\ta_words = random_deletion(words, p_rd)\n",
        "\t\t\taugmented_sentences.append(' '.join(a_words))\n",
        "\n",
        "\taugmented_sentences = [get_only_chars(sentence) for sentence in augmented_sentences]\n",
        "\tshuffle(augmented_sentences)\n",
        "\n",
        "\t#trim so that we have the desired number of augmented sentences\n",
        "\tif num_aug >= 1:\n",
        "\t\taugmented_sentences = augmented_sentences[:num_aug]\n",
        "\telse:\n",
        "\t\tkeep_prob = num_aug / len(augmented_sentences)\n",
        "\t\taugmented_sentences = [s for s in augmented_sentences if random.uniform(0, 1) < keep_prob]\n",
        "\n",
        "\t#append the original sentence\n",
        "\taugmented_sentences.append(sentence)\n",
        "\n",
        "\treturn augmented_sentences"
      ],
      "metadata": {
        "id": "ExFD_OBxZO2u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def any2past_augment(train_orig, output_file, alpha_sr, alpha_ri, alpha_rs, alpha_rd, num_aug=9):\n",
        "  writer = open(output_file, 'w')\n",
        "  lines = open(train_orig, 'r').readlines()\n",
        "\n",
        "  for i, line in enumerate(lines):\n",
        "        parts = line[:-1].split('\\t')\n",
        "        sentence = parts[0]\n",
        "        aug_sentences = eda(sentence, alpha_sr=alpha_sr, alpha_ri=alpha_ri, alpha_rs=alpha_rs, p_rd=alpha_rd, num_aug=num_aug)\n",
        "        for aug_sentence in aug_sentences:\n",
        "            result = change_tense(aug_sentence, 'past')\n",
        "            writer.write(\"\\t\" + result + '\\n')\n",
        "\n",
        "  writer.close()\n",
        "  print(\"generated augmented sentences with eda for \" + train_orig + \" to \" + output_file + \" with num_aug=\" + str(num_aug))"
      ],
      "metadata": {
        "id": "ojdH2_sVcCST"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#the output file\n",
        "output = None\n",
        "input = '/content/test.txt'\n",
        "if output:\n",
        "    output = output\n",
        "else:\n",
        "    from os.path import dirname, basename, join\n",
        "    output = join(dirname(input), 'eda_' + basename(input))\n",
        "\n",
        "#number of augmented sentences to generate per original sentence\n",
        "num_aug = 9 #default\n",
        "if num_aug:\n",
        "    num_aug = num_aug\n",
        "\n",
        "#how much to replace each word by synonyms\n",
        "alpha_sr = 0.1#default\n",
        "if alpha_sr is not None:\n",
        "    alpha_sr = alpha_sr\n",
        "\n",
        "#how much to insert new words that are synonyms\n",
        "alpha_ri = 0.1#default\n",
        "if alpha_ri is not None:\n",
        "    alpha_ri = alpha_ri\n",
        "\n",
        "#how much to swap words\n",
        "alpha_rs = 0.1#default\n",
        "if alpha_rs is not None:\n",
        "    alpha_rs = alpha_rs\n",
        "\n",
        "#how much to delete words\n",
        "alpha_rd = 0.1#default\n",
        "if alpha_rd is not None:\n",
        "    alpha_rd = alpha_rd\n",
        "\n",
        "# if alpha_sr == alpha_ri == alpha_rs == alpha_rd == 0:\n",
        "#      ap.error('At least one alpha should be greater than zero')"
      ],
      "metadata": {
        "id": "A70SyM1VdJeb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#generate augmented sentences and output into a new file\n",
        "any2past_augment(input, output, alpha_sr=alpha_sr, alpha_ri=alpha_ri, alpha_rs=alpha_rs, alpha_rd=alpha_rd, num_aug=num_aug)"
      ],
      "metadata": {
        "id": "Hjplxl0Cc8jA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RhH7x5qxdUpO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}